{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44f6fc8",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db5bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "from scipy.stats import loguniform, randint\n",
    "from yellowbrick.model_selection import RFECV\n",
    "from scipy.stats import loguniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0adb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import cuml\n",
    "# from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import cudf\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f53ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directory\n",
    "os.chdir(\"/home/siti/Downloads/New Modelling\")\n",
    "\n",
    "# Load the CSV data into a pandas DataFrame\n",
    "df = pd.read_csv('AE_Synthetic_Data_Final_V02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b644e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df[\"LOS_Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51379daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = Path(\"LogisticRegression_folder/Version 2/Model 1\")\n",
    "results_folder.mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2959d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object types to category\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c408d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of df for modelling\n",
    "data = df.copy()\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237df6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target\n",
    "feature_cols = data.columns[data.columns != 'LOS_Category']\n",
    "X = data[feature_cols]\n",
    "y = data['LOS_Category']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af639608",
   "metadata": {},
   "source": [
    "# Step 1: Map the Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b93086",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.map({'Short stay': 1, 'Long stay': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd12da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffcd29",
   "metadata": {},
   "source": [
    "# Step 2: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d9e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbae354",
   "metadata": {},
   "source": [
    "**Overall Proportions:**\n",
    "\n",
    "- **Test set**: 15% of the original dataset\n",
    "- **Validation set**: 0.1765 * 0.85 = 0.15 or 15% of the original dataset\n",
    "- **Training set**: 1−0.15−0.15 = 0.7 or 70% of the original dataset\n",
    "\n",
    "So, by using this two-step splitting, we've achieved a 70-15-15 split for the training, validation, and test datasets, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1d099",
   "metadata": {},
   "source": [
    "# Step 3: One-Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98591082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your nominal columns\n",
    "nominal_columns = ['Sex', 'AE_Arrival_Mode', 'ICD10_Chapter_Code', 'TFC']\n",
    "\n",
    "# One-hot encoding\n",
    "X_train = pd.get_dummies(X_train, columns=nominal_columns, drop_first=True)\n",
    "X_val = pd.get_dummies(X_val, columns=nominal_columns, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=nominal_columns, drop_first=True)\n",
    "\n",
    "# Get all columns in X_train after one-hot encoding\n",
    "all_columns = X_train.columns\n",
    "\n",
    "# Ensure X_val has the same columns as X_train\n",
    "for col in all_columns:\n",
    "    if col not in X_val.columns:\n",
    "        X_val[col] = 0\n",
    "\n",
    "# Ensure X_test has the same columns as X_train\n",
    "for col in all_columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "\n",
    "# Reorder columns in X_val and X_test to match X_train's order\n",
    "X_val = X_val[all_columns]\n",
    "X_test = X_test[all_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b083554",
   "metadata": {},
   "source": [
    "# Step 4: Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = {\n",
    "    'IMD_Category': ['Most deprived 20%', '20% - 40%', '40 - 60%', '60% - 80%', 'Least deprived 20%'],\n",
    "    'Age_Band': ['01-17', '18-24', '25-44', '45-64', '65-84', '85+'],\n",
    "    'AE_Arrive_HourOfDay': ['01-04','05-08','09-12','13-16','17-20','21-24'],\n",
    "    'AE_HRG': ['Nothing', 'Low', 'Medium', 'High']\n",
    "}\n",
    "\n",
    "# Loop through each column and its respective order to encode\n",
    "for col, order in orders.items():\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[order])\n",
    "    X_train[col] = ordinal_encoder.fit_transform(X_train[[col]])\n",
    "    X_val[col] = ordinal_encoder.transform(X_val[[col]])\n",
    "    X_test[col] = ordinal_encoder.transform(X_test[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534a0c0",
   "metadata": {},
   "source": [
    "# Step 5: Oversampling with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298aea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the oversampled data from the CSV file\n",
    "df_loaded = pd.read_csv('model1_oversampled_data.csv')\n",
    "\n",
    "# Separate the features and target variable\n",
    "X_train_resampled = df_loaded.drop('LOS_Category', axis=1)\n",
    "y_train_resampled = df_loaded['LOS_Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = y_train_resampled.value_counts(normalize=True)\n",
    "print(proportions)\n",
    "\n",
    "counts = y_train_resampled.value_counts(normalize=False)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c585ee0-b7ae-4736-8418-ea14dc4cad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Use only 10% of the data for hyperparameter tuning using stratified sampling\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "for train_index, subset_index in sss.split(X_train_resampled, y_train_resampled):\n",
    "    X_train_subset = X_train_resampled.iloc[subset_index]\n",
    "    y_train_subset = y_train_resampled.iloc[subset_index]\n",
    "    \n",
    "# Standardize the subset\n",
    "scaler = StandardScaler()\n",
    "X_train_subset_scaled = scaler.fit_transform(X_train_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb21b55-7968-4894-8f40-dbe024d649bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_train_subset.value_counts(normalize=False)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de50c1",
   "metadata": {},
   "source": [
    "# Step 6: Standardize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f000b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the rest of the data using the subset's statistics\n",
    "X_train_scaled = scaler.transform(X_train_resampled)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da3780",
   "metadata": {},
   "source": [
    "# Step 7: RFECV\n",
    "\n",
    "Perform RFECV on the training set (not including the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643f689-a62a-40ce-87ee-17557cce25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5af95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Initialize the Logistic Regression estimator\n",
    "estimator = LogisticRegression(max_iter=5000, random_state=42)\n",
    "\n",
    "# Use RFECV from Yellowbrick with the settings\n",
    "visualizer = RFECV(estimator, step=1, cv=5, scoring = 'roc_auc')\n",
    "\n",
    "# Assuming X_train_subset_scaled has been pre-scaled. If not, scale it first.\n",
    "visualizer.fit(X_train_subset_scaled, y_train_subset)\n",
    "\n",
    "# Set the title, x-label, and y-label before saving the figure\n",
    "plt.title(\"RFECV - Feature Selection with Logistic Regression\")\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"Cross-Validation Score\")\n",
    "plt.savefig(\"LR_GPU_V02.png\", dpi=300)\n",
    "\n",
    "# Show the visualizer\n",
    "visualizer.show(outpath= results_folder / \"RFECV.png\")\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the duration\n",
    "duration = end_time - start_time\n",
    "print(f\"RFECV took {duration:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f368a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the features selected by RFECV\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Map the selected features to the original column names\n",
    "selected_features = [feature for mask, feature in zip(visualizer.support_, feature_names) if mask]\n",
    "\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0016266",
   "metadata": {},
   "source": [
    "# Step 8: Hyperparameter Tuning with Randomized Search CV\n",
    "\n",
    "After feature selection, use RandomizedSearchCV for hyperparameter tuning on the training set. The cross-validation within RandomizedSearchCV uses multiple train-validation splits of the training data.\n",
    "\n",
    "Use only 10% percent of the training set to perform hyperparameter tuning due to insuffient CPU and GPU resources. The training set is very high after oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845e4952-4df1-49e8-aa24-fe0db041db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['IMD_Category', 'Age_Band', 'AE_Arrive_HourOfDay', 'AE_Time_Mins', 'AE_HRG', 'AE_Num_Diagnoses', 'AE_Num_Investigations', 'AE_Num_Treatments', 'Sex_Male', 'AE_Arrival_Mode_Not known', 'AE_Arrival_Mode_Other', 'ICD10_Chapter_Code_Other', 'ICD10_Chapter_Code_X', 'ICD10_Chapter_Code_XI', 'ICD10_Chapter_Code_XIV', 'ICD10_Chapter_Code_XIX', 'ICD10_Chapter_Code_XVIII', 'TFC_180', 'TFC_300', 'TFC_420', 'TFC_OTHER']\n",
    "# Extract columns from the scaled and resampled training set using the list of selected features\n",
    "selected_feature_indices = [X_train.columns.get_loc(feature) for feature in selected_features]\n",
    "X_train_selected_scaled = X_train_scaled[:, selected_feature_indices]\n",
    "\n",
    "# Filter X_train_subset_scaled with the best features\n",
    "X_train_selected_subset_scaled = X_train_subset_scaled[:, selected_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression estimator\n",
    "estimator = LogisticRegression(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'C': loguniform(1e-3, 1e2),\n",
    "    'penalty': ['l2'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'solver': ['newton-cg', 'lbfgs','sag'],\n",
    "    'max_iter': randint(100, 5000)\n",
    "}\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator, param_distributions=param_dist, \n",
    "                                   scoring = 'roc_auc', error_score='raise',\n",
    "                                   n_iter=50, cv=5, verbose=1, n_jobs=7, random_state=42)\n",
    "\n",
    "random_search.fit(X_train_selected_subset_scaled, y_train_subset)\n",
    "\n",
    "# Save the cv_results_ to a DataFrame and then to a CSV file\n",
    "cv_results_df = pd.DataFrame(random_search.cv_results_)\n",
    "cv_results_df.to_csv(results_folder / 'cv_results.csv', index=False)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters for LR:\", random_search.best_params_)\n",
    "print(\"Best ROC-AUC score LR:\", random_search.best_score_)\n",
    "\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Hyperparameter tuning for LR took {duration:.2f} seconds.\")\n",
    "\n",
    "# Save the model\n",
    "dump(random_search, results_folder / 'random_search_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3f687",
   "metadata": {},
   "source": [
    "# Step 9: Train the whole dataset with best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting selected features for validation, and test sets:\n",
    "\n",
    "X_val_selected_scaled = X_val_scaled[:, selected_feature_indices]\n",
    "X_test_selected_scaled = X_test_scaled[:, selected_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train a DT model using the best hyperparameters on the training set\n",
    "best_model = LogisticRegression(**best_params)\n",
    "best_model.fit(X_train_selected_scaled, y_train_resampled)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training for LR took {duration:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0f7f0",
   "metadata": {},
   "source": [
    "# Step 10: Predict and Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = best_model.predict(X_val_selected_scaled)\n",
    "y_val_prob = best_model.predict_proba(X_val_selected_scaled)[:, 1]\n",
    "\n",
    "# Evaluate the performance on the validation set using multiple metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_prob)  # Note: roc_auc_score uses probability estimates\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"Accuracy:\", val_accuracy)\n",
    "print(\"ROC-AUC:\", val_roc_auc)\n",
    "print(\"F1 Score:\", val_f1)\n",
    "print(\"Precision:\", val_precision)\n",
    "print(\"Recall:\", val_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87835641",
   "metadata": {},
   "source": [
    "# Step 11: Final Model Training and Test Evaluation\n",
    "\n",
    "Finally, we train on the combined train+validation data and test on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16730506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine the training and validation sets\n",
    "X_combined = np.concatenate([X_train_selected_scaled, X_val_selected_scaled], axis=0)\n",
    "y_combined = np.concatenate([y_train_resampled, y_val], axis=0)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the final model on the combined set\n",
    "final_model = LogisticRegression(**best_params)\n",
    "final_model.fit(X_combined, y_combined)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"Training LR took {duration:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73996f7b",
   "metadata": {},
   "source": [
    "# Step 12: Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d99f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = final_model.predict(X_test_selected_scaled)\n",
    "y_test_prob = final_model.predict_proba(X_test_selected_scaled)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Evaluate the performance on the test set using multiple metrics\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Accuracy:\", test_accuracy)\n",
    "print(\"ROC-AUC:\", test_roc_auc)\n",
    "print(\"F1 Score:\", test_f1)\n",
    "print(\"Precision:\", test_precision)\n",
    "print(\"Recall:\", test_recall)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db2f88",
   "metadata": {},
   "source": [
    "# Step 13: Save the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df87273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all metrics in a csv file\n",
    "\n",
    "metrics = {\n",
    "    \"ROC-AUC Test\": test_roc_auc ,\n",
    "    \"Accuracy\": test_accuracy,\n",
    "    \"Precision\": test_precision,\n",
    "    \"Recall\": test_recall,\n",
    "    \"F1 Score\": test_f1,\n",
    "    \"Confusion Matrix\": test_conf_matrix\n",
    "}\n",
    "\n",
    "# Convert the metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Save to CSV\n",
    "output_path = results_folder / \"Model1_Performance_metrics.csv\"\n",
    "metrics_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b0210",
   "metadata": {},
   "source": [
    "# ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# y_test_prob contains the predicted probabilities for the positive class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(results_folder / \"roc_auc.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832d3fc",
   "metadata": {},
   "source": [
    "# Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, y_test_prob)\n",
    "average_precision = average_precision_score(y_test, y_test_prob)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label='AP (Area = %0.3f)' % average_precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(results_folder / \"precision_recall.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e263aaa",
   "metadata": {},
   "source": [
    "# Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_combined_pred = final_model.predict(X_combined)\n",
    "combined_accuracy = accuracy_score(y_combined, y_combined_pred)\n",
    "combined_error = 1 - combined_accuracy\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = final_model.predict(X_val_selected_scaled)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_error = 1 - val_accuracy\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = final_model.predict(X_test_selected_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_error = 1 - test_accuracy\n",
    "\n",
    "print(f\"Combined Error: {combined_error:.3f}\")\n",
    "print(f\"Validation Error: {val_error:.3f}\")\n",
    "print(f\"Test Error: {test_error:.3f}\")\n",
    "\n",
    "# Create a DataFrame to store the metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Combined Error', 'Validation Error', 'Test Error'],\n",
    "    'Value': [combined_error, val_error, test_error]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(results_folder / 'Model_Metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = final_model.coef_[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95374886",
   "metadata": {},
   "source": [
    "## Calculate Odds Ratios\n",
    "The odds ratio for a feature represents how the odds of the positive class change with a one-unit increase in that feature while holding other features constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratios = np.exp(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d5a39",
   "metadata": {},
   "source": [
    "## Rank Features Base don Importance\n",
    "\n",
    "Rank the features based on the magnitude of their coefficients. The magnitude indicates the importance of the feature, regardless of its direction (positive or negative effect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Feature Importance to Feature Names:\n",
    "feature_names = [X_train.columns[i] for i in selected_feature_indices]\n",
    "\n",
    "sorted_indices = np.argsort(np.abs(coefficients))[::-1]\n",
    "\n",
    "ranked_features = [feature_names[i] for i in sorted_indices]\n",
    "ranked_coefficients = [coefficients[i] for i in sorted_indices]\n",
    "ranked_odds_ratios = [odds_ratios[i] for i in sorted_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daaa6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(ranked_features, ranked_coefficients)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Feature Importance based on Coefficients')\n",
    "plt.gca().invert_yaxis()  # To display the most important feature at the top\n",
    "plt.savefig(results_folder / \"Feature_Importance_coeff.png\", dpi=300)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0e8d3",
   "metadata": {},
   "source": [
    "A positive coefficient suggests that as the feature value increases, the log-odds of the response variable being 1 also increase, making the response more likely to be 1. Conversely, a negative coefficient suggests the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.barh(ranked_features, ranked_odds_ratios, color='skyblue')\n",
    "plt.xlabel('Odds Ratio')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Odds Ratios of Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(results_folder / \"Feature_Importance_odds.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ba3f6",
   "metadata": {},
   "source": [
    "An odds ratio greater than 1 indicates that the feature has a positive relationship with the outcome. If the odds ratio is less than 1, the feature has a negative relationship with the outcome. An odds ratio of 1 suggests that the feature has no effect on the outcome. For example, an odds ratio of 2 for a given feature means that with a one-unit increase in that feature, the odds of the outcome being 1 are doubled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c6b05-bf05-4be8-bf86-4221501c2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Feature': ranked_features,\n",
    "    'Coefficient': ranked_coefficients,\n",
    "    'Odds Ratio': ranked_odds_ratios\n",
    "})\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df.to_csv(results_folder / 'logistic_regression_coefficients.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ffd5db",
   "metadata": {},
   "source": [
    "# Significance of Features to Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b139b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected_scaled_df = pd.DataFrame(X_train_selected_scaled, columns=selected_features)\n",
    "X_val_selected_scaled_df = pd.DataFrame(X_val_selected_scaled, columns=selected_features)\n",
    "\n",
    "# Combine the training and validation sets\n",
    "X_combined = pd.concat([X_train_selected_scaled_df, X_val_selected_scaled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729db6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_intercept = sm.add_constant(X_combined)\n",
    "model = sm.Logit(y_combined, X_with_intercept).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245feeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summary as a dataframe\n",
    "# Get the summary as a DataFrame\n",
    "summary_df = model.summary2().tables[1]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "summary_df.to_csv(results_folder / 'Model1_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901d3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
